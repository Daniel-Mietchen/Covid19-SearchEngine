{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is completaly based on https://www.kaggle.com/dgunning/browsing-research-papers-with-a-bm25-search-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing rank_bm25 library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joseh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pathlib import Path, PurePath\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import HTTPError, ConnectionError\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"punkt\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some adjustments in display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7692a0ee4901460abcb2ea89b466ba81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=200, description='ColumnWidth', max=400, min=50, step=50), IntSlider(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "\n",
    "def set_column_width(ColumnWidth, MaxRows):\n",
    "    pd.options.display.max_colwidth = ColumnWidth\n",
    "    pd.options.display.max_rows = MaxRows\n",
    "    print('Set pandas dataframe column width to', ColumnWidth, 'and max rows to', MaxRows)\n",
    "    \n",
    "interact(set_column_width, \n",
    "         ColumnWidth=widgets.IntSlider(min=50, max=400, step=50, value=200),\n",
    "         MaxRows=widgets.IntSlider(min=50, max=500, step=100, value=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are all the files located\n",
    "input_dir = PurePath('2020-03-13')\n",
    "\n",
    "# The all sources metadata file\n",
    "metadata = pd.read_csv(input_dir / 'all_sources_metadata_2020-03-13.csv',\n",
    "                       dtype={'Microsoft Academic Paper ID': str,\n",
    "                             'pubmed_id': str})\n",
    "\n",
    "# Convert the doi to a url\n",
    "def doi_url(d): return f'http://{d}' if d.startswith('doi.org') else f'http://doi.org/{d}'\n",
    "metadata.doi = metadata.doi.fillna('').apply(doi_url)\n",
    "\n",
    "# Set the abstract to the paper title if it is null\n",
    "metadata.abstract = metadata.abstract.fillna(metadata.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of papers on metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicate papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some papers are duplicated since they were collected from separate sources. Thanks Joerg Rings\n",
    "duplicate_paper = ~(metadata.title.isnull() | metadata.abstract.isnull()) & (metadata.duplicated(subset=['title', 'abstract']))\n",
    "metadata = metadata[~duplicate_paper].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results after deleting duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Class to process the Research Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class with function that help handle papers information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(url, timeout=6):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        return r.text\n",
    "    except ConnectionError:\n",
    "        print(f'Cannot connect to {url}')\n",
    "        print(f'Remember to turn Internet ON in the Kaggle notebook settings')\n",
    "    except HTTPError:\n",
    "        print('Got http error', r.status, r.text)\n",
    "\n",
    "class DataHolder:\n",
    "    '''\n",
    "    A wrapper for a dataframe with useful functions for notebooks\n",
    "    '''\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, item): return self.data.loc[item]\n",
    "    def head(self, n:int): return DataHolder(self.data.head(n).copy())\n",
    "    def tail(self, n:int): return DataHolder(self.data.tail(n).copy())\n",
    "    def _repr_html_(self): return self.data._repr_html_()\n",
    "    def __repr__(self): return self.data.__repr__()\n",
    "\n",
    "\n",
    "class ResearchPapers:\n",
    "    \n",
    "    def __init__(self, metadata: pd.DataFrame):\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return Paper(self.metadata.iloc[item])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def head(self, n):\n",
    "        return ResearchPapers(self.metadata.head(n).copy().reset_index(drop=True))\n",
    "    \n",
    "    def tail(self, n):\n",
    "        return ResearchPapers(self.metadata.tail(n).copy().reset_index(drop=True))\n",
    "    \n",
    "    def abstracts(self):\n",
    "        return self.metadata.abstract.dropna()\n",
    "    \n",
    "    def titles(self):\n",
    "        return self.metadata.title.dropna()\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.metadata._repr_html_()\n",
    "    \n",
    "class Paper:\n",
    "    \n",
    "    '''\n",
    "    A single research paper\n",
    "    '''\n",
    "    def __init__(self, item):\n",
    "        self.paper = item.to_frame().fillna('')\n",
    "        self.paper.columns = ['Value']\n",
    "    \n",
    "    def doi(self):\n",
    "        return self.paper.loc['doi'].values[0]\n",
    "    \n",
    "    def html(self):\n",
    "        '''\n",
    "        Load the paper from doi.org and display as HTML. Requires internet to be ON\n",
    "        '''\n",
    "        text = get(self.doi())\n",
    "        return widgets.HTML(text)\n",
    "    \n",
    "    def text(self):\n",
    "        '''\n",
    "        Load the paper from doi.org and display as text. Requires Internet to be ON\n",
    "        '''\n",
    "        text = get(self.doi())\n",
    "        return text\n",
    "    \n",
    "    def abstract(self):\n",
    "        return self.paper.loc['abstract'].values[0]\n",
    "    \n",
    "    def title(self):\n",
    "        return self.paper.loc['title'].values[0]\n",
    "    \n",
    "    def authors(self, split=False):\n",
    "        '''\n",
    "        Get a list of authors\n",
    "        '''\n",
    "        authors = self.paper.loc['authors'].values[0]\n",
    "        if not authors:\n",
    "            return []\n",
    "        if not split:\n",
    "            return authors\n",
    "        if authors.startswith('['):\n",
    "            authors = authors.lstrip('[').rstrip(']')\n",
    "            return [a.strip().replace(\"\\'\", \"\") for a in authors.split(\"\\',\")]\n",
    "        \n",
    "        # Todo: Handle cases where author names are separated by \",\"\n",
    "        return [a.strip() for a in authors.split(';')]\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.paper._repr_html_()\n",
    "    \n",
    "\n",
    "papers = ResearchPapers(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search index for matching tokens in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing\n",
    "To prepare the text for the search index we perform the following steps\n",
    "\n",
    "1. Remove punctuations and special characters\n",
    "2. Convert to lowercase\n",
    "3. Tokenize into individual tokens (words mostly)\n",
    "4. Remove stopwords like (and, to))\n",
    "\n",
    "You can tweak the code below to improve the search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joseh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "def strip_characters(text):\n",
    "    t = re.sub('\\(|\\)|:|,|;|\\.|’|”|“|\\?|%|>|<', '', text)\n",
    "    t = re.sub('/', ' ', t)\n",
    "    t = t.replace(\"'\",'')\n",
    "    return t\n",
    "\n",
    "def clean(text):\n",
    "    t = text.lower()\n",
    "    t = strip_characters(t)\n",
    "    return t\n",
    "\n",
    "def tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return list(set([word for word in words \n",
    "                     if len(word) > 1\n",
    "                     and not word in english_stopwords\n",
    "                     and not (word.isnumeric() and len(word) is not 4)\n",
    "                     and (not word.isnumeric() or word.isalpha())] )\n",
    "               )\n",
    "\n",
    "def preprocess(text):\n",
    "    t = clean(text)\n",
    "    tokens = tokenize(t)\n",
    "    return tokens\n",
    "\n",
    "class SearchResults:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame,\n",
    "                 columns = None):\n",
    "        self.results = data\n",
    "        if columns:\n",
    "            self.results = self.results[columns]\n",
    "            \n",
    "    def __getitem__(self, item):\n",
    "        return Paper(self.results.loc[item])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.results)\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.results._repr_html_()\n",
    "\n",
    "SEARCH_DISPLAY_COLUMNS = ['title', 'abstract', 'doi', 'authors', 'journal']\n",
    "\n",
    "class WordTokenIndex:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 corpus: pd.DataFrame, \n",
    "                 columns=SEARCH_DISPLAY_COLUMNS):\n",
    "        self.corpus = corpus\n",
    "        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('')\n",
    "        self.index = raw_search_str.apply(preprocess).to_frame()\n",
    "        self.index.columns = ['terms']\n",
    "        self.index.index = self.corpus.index\n",
    "        self.columns = columns\n",
    "    \n",
    "    def search(self, search_string):\n",
    "        search_terms = preprocess(search_string)\n",
    "        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n",
    "        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n",
    "        return SearchResults(results, self.columns + ['paper'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using rankBM25 search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankBM25Index(WordTokenIndex):\n",
    "    \n",
    "    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n",
    "        super().__init__(corpus, columns)\n",
    "        self.bm25 = BM25Okapi(self.index.terms.tolist())\n",
    "        \n",
    "    def search(self, search_string, n=10):\n",
    "        search_terms = preprocess(search_string)\n",
    "        doc_scores = self.bm25.get_scores(search_terms)\n",
    "        ind = np.argsort(doc_scores)[::-1][:n]\n",
    "        results = self.corpus.iloc[ind][self.columns]\n",
    "        results['Score'] = doc_scores[ind]\n",
    "        results = results[results.Score > 0]\n",
    "        return SearchResults(results.reset_index(), self.columns + ['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_index = RankBM25Index(metadata.head(len(metadata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.RankBM25Index object at 0x000001CC661569B0>\n"
     ]
    }
   ],
   "source": [
    "print(repr(bm25_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('mypickle.pickle', 'wb') as f:\n",
    "    pickle.dump(bm25_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Creating autocomplete text bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mypickle.pickle', 'rb') as f:\n",
    "    loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ea35db77234c8c80cb9d853507f3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='mexico', description='SearchTerms'), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def search_papers(SearchTerms: str):\n",
    "    search_results = loaded.search(SearchTerms, n=100)\n",
    "    if len(search_results) > 0:\n",
    "        display(search_results) \n",
    "    return search_results\n",
    "\n",
    "searchbar = widgets.interactive(search_papers, SearchTerms='mexico')\n",
    "display(searchbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Paper"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = bm25_index.search('Panama', n=100)\n",
    "type(search_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://doi.org/'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0].doi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder(JSONEncoder):\n",
    "        def default(self, o):\n",
    "            return o.__dict__    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"paper\": {\"_is_copy\": null, \"_data\": {}, \"_item_cache\": {}}}'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyEncoder().encode(search_results[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
